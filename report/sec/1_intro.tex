\section{Introduction}
\label{sec:intro}

최근 딥러닝 모델은 대규모 데이터에 기반해 뛰어난 성능을 달성하고 있지만, 훈련(source) 환경과 배포(target) 환경 사이의 도메인 편차(domain shift)가 존재할 때 성능이 급격히 저하되는 한계를 드러낸다. 이러한 문제를 해결하고자 제안된 task 가 바로 도메인 일반화(Domain Generalization, DG) 이며, 해당 task 는 학습 단계에서 접하지 않은 도메인에서도 일관된 성능을 유지하는 표현을 학습하는 것을 목적으로 하며 본 보고서에서는,



1. ResNet-50 기반 Baseline,

2. 통계적 정렬 계열 기법인 MMD,

3. 주파수 기반 증강 기법인 FACT,

4. 우리가 제안하는 Cosine Similarity 정렬 손실,


5. 우리가 제안하는 FPJ-GC


다섯 가지 방법을 동일한 실험 조건에서 비교한다. 각 방법의 이론적 배경과 구현 세부를 설명하고, PACS 데이터셋 및 비공개 Real 도메인에서의 실험 결과를 통해 제안 기법의 우수성과 안정성을 검증하는 것을 목표로 한다

%-------------------------------------------------------------------------
\section{Related Works}

해당 섹션에서는 team project reference 에 기반한 2가지의 기존 일반화 방법인, 

•  MMD

•  FACT

2가지 방법에 대해서 설명하고자 한다.

\subsection{MMD}

Team project reference 논문 중 하나인 “Domain Generalization via Invariant Feature Representation” 의 일반화 방법이며 도메인 간 global alignment 를 잘 수행하여 각 객체에 대한 invariant feature representation 을 획득해 일반화를 수행하는 것을 목표로 한다.

\subsubsection{Main Idea}
서로 다른 소스 도메인들의 feature distribution 을 kernel space 에서 일치시키게 된다면,보지 못한 도메인에서도 분포가 크게 어긋나지 않아 모델이 instance 에 대한 invariant feature representation 학습하게 된다.

\subsubsection{Method}
기존 kernel space 의 feature 를 gaussian kernel 로 mapping 하여 mean embedding 을 구하고 모든 도메인 쌍에 대해 차를 합산. 이를 이용하여 두 feature distribution 을 일치시키는 방법론이다.

\subsection{FACT}

Team project reference 논문 중 하나인 “A Fourier-based Framework for Domain Generalization” 의 일반화 방법이며 저주파 진폭은 도메인 특유의 정보를 많이 담고있다는 걸 이용하여 저주파 진폭에 대한 가짜 도메인을 FFT 를 통해 증강하여 네트워크가 저주파 진폭에 대한 의존도를 줄이고 위상에 대한 의존도를 높이게끔 유도하여 invariant representation 을 습득하고자 하는 방법론이다.

\subsubsection{Main Idea}
이미지의 위상 (phase) 는 고차원적인 의미를 담고, 저주파 진폭 (low-freq amplitude) 는 도메인의 조명, 스타일 같은 각 도메인 특유의 정보가 많다. 따라서, 이런 특성을 지닌 저주파 진폭을 섞어 가짜 도메인을 만들어 네트워크에 perturbation 을 주게 된다면 도메인 특유에 대한 정보에 노이즈가 심해져 의존도가 기존보다 줄어 들고 객체의 형태나 고유 정보를 포함하고 있는 위상정보에 더 의존하게 되는 invariant representation 을 학습할 수 있다.

\subsubsection{Method}
두 이미지를 Fast fourier transform (FFT) 로 분해하여 진폭과 위상을 분리하여 저주파 영역에 선형보간을 수행해 amplitude 에 mix 을 수행한다. 이후, inv FFT 를 통해 공간 도메인을 복원하고 student/teacher 에 동시 입력하여 원본과 mix 간 이미지 예측값이 모두 일관되도록 수행한다.


\section{Proposed methods}
해당 섹션에서는 우리가 제안하는 방법론인

• cosine similarity
• FPJ-GC


2가지 방법에 대해서 설명하고자 한다.

\subsection{Cosine similarity}

기존 MMD 는 gaussian kernel 을 통해 변환된 feature 를 서로 근사시키는데 반해, 해당 방법은 gaussian kernel 을 cosine simiarity 로 바꿈으로써 gaussian kernel 의 hyper parameter choice 에서도 자유롭고, 직접적으로 벡터의 방향만 비교함으로써 같은 클래스들을 도메인과 관계없이 보다 잘 정렬시켜 일반화 능력을 강화한다.

\subsubsection{Method}
Gaussian kernel 을 cosine similarity 로 변환하여 mmd 를 수행한다.


\subsection{Fourier-Phase Jitter with Graph Consistency}

Phase 스펙트럼에 저/고주파별 미세 노이즈를 가해 style-invariant 구조 변형을 만들고, 원본 배치와 jitter 배치가 동일한 cosine simialrity graph 를 유지하도록 학습하게 됨으로써 도메인 변화에 보다 강인한 feature 를 획득하게끔 하는 방법론이다.

\subsubsection{Main Idea}
Amplitude (FACT, Mix-style) 과 달리 phase 를 조작하며 개별 샘플 일치 대신 배치-전역 그래프 일관성으로 보다 안정적으로 수렴하게 하여 강인한 feature 를 획득하여 일반화 능력을 강화한다.

\subsubsection{Pros}
Phase 중심 변조를 통해 shape-shift 도메인에 강하고 배치레벨 그래프를 통해 개별 alignment 보다 안정적이게 되면서 자연스레 collapse 위험도가 낮게 되고, 기존 방식들에서 추가적인 parameter 를 요구하지 않기에 computation cost 증가에서 자유롭다.

\subsubsection{Differences from existing methodologies}
amplitude 변환을 통해 도메인에 대한 정보 의존도를 줄여 일반화 능력을 강화하는 기존의 방식들과 다르게, phase jitter 를 통해 보다 robust 한 특성을 직접적으로 습득하면서 Graph consistency 를 원본과 변형의 관계 유지용으로 사용하기에 기존 연구와의 차이가 있다.


\section{Experiments}
해당 섹션은 위에서 설명한 방법론들과 base 모델 간 비교를 통해 성능비교를 수행한다. 

\subsection{Baseline}
assignment 기본 설정에서 어떠한 변화를 주지 않고 classification 을 수행하며, 이후 실험 또한 추가되는 방법에 대한 모듈을 제외한 모든 실험 환경은 baseline 과 동일하다.

\subsection{Method module config}
MMD : sigma = (1, 2, 4, 8), lambda = 0.1

FACT : eta = 0.3, beta = 0.25, temperature= 3.0 

Cos sim : lambda = 1.0

FPJ-GC : lambda graph = 0.2

\subsection{Dataset}
학습에는 기존 실험세팅에서 주어진 pre-trained weight 와 PACS dataset 을 사용했으나, test 셋은 PACS 와 동일한 카테고리를 지닌 실제 이미지들을 크롤링하여 50장씩 구성한 CuratedPACS 를 따로 구축하여 성능비교 데이터셋으로 사용함. 

\begin{table}
    \centering
    \begin{tabular}{ccc}
             &  실험 1 & 실험 2 \\
        Base&  69.40 \%&66.53\% \\
         MMD&  67.96\%&71.92\% \\
         FACT&  80.90\%&77.25\% \\
         Cos Sim&  81.43\%&78.86\% \\
         FPJ-GC&  81.83\%&82.09\% \\
    \end{tabular}
    \caption{Results}
\end{table}

\subsection{Analysis}
모든 제안 기법이 Baseline 대비 일관된 성능 향상을 달성하였다. 다만 다음 세 가지 실험적 제약을 고려할 필요가 있다.

1. Curated PACS 표본 수의 한계
: 평가 세트가 작아 통계적 신뢰 구간이 넓다.

2. PACS 자체의 소규모 규모
: 도메인 다양성이 제한적이어서 실제 배포 환경 전체를 대표하기 어렵다.

3. 10 epoch로 제한된 학습 스케줄
:충분한 수렴 이전 단계에서 모델 간 격차가 과소·과대평가될 수 있다.

따라서 이번 결과를 절대적인 일반화 능력 차이로 단정하긴 이르다. 그럼에도 동일한 조건에서 관찰된 유의미한 우위는 제안 기법들이 도메인 일반화에 실질적 효과가 있음을 강하게 시사한다. 더 큰 데이터셋과 장기 학습을 수행하게 된다면 보다 안정화된 데이터를 얻을 수 있을 것으로 보여진다.

\section{Discussion / Conclusion}
\subsection{실험을 여러번 돌린 이유}
단일 실행으로는 결과 해석이 어렵다는 사실이 초기 탐색에서 드러났다.
예를 들어 MMD 모델은 동일한 조건에서 시드만 달라졌을 때 정확도가 10 \%p 이상 변동했다. 이런 편차가 존재하면, 새 모듈이 가져온 성능 변화가 실제 일반화 효과인지, 아니면 훈련 불안정성 때문인지 구분하기 힘들다.
우리는 변동 원인으로 Color Jitter 등은 매 미니배치마다 다른 색 공간 변형을 주어 일반화에 도움을 주지만, 증강 분산을 모델이 충분히 흡수·평균화하기엔 학습 시간이 짧기에 증강 편차가 그대로 최종 성능의 노이즈로써 남게되는 것으로 추측하였다. 이에 대한 해결책으로 증강 시드 고정을 검토했으나 모델에 편향성을 학습시킬 위험과 과제에서 제시한 기본 설정을 변경하게 되므로 실험에 영향을 줄 수 있기에 채택하지 않았다. 다른 방법으로 우리는 여러번의 실험을 통해 평균적인 모델 성능을 구하여 방법론 자체가 가져온 효과를 보다 신뢰성 있게 평가하는 방식을 선택했다.

\subsection{FPJ-GC 모델 선정 이유}
Curated PACS 평가에서는 Cosine Similarity와 FPJ-GC가 모두 높은 정확도를 보였다. 그러나 Cosine Similarity 모듈은 MMD 효과를 강화하려고 유사도 손실 가중치(λ)를 높여야만성능이 극대화됐다. 이 과정에서 하이퍼 파라미터에 대한 민감도가 증가하여 재현성이 떨어질 수 있다는 점과 성능 변동의 폭이 FPJ-GC 가 보다 작았으며 추가조정 또한 없었다는 점을 들어 최종선택을 하게 되었다.

\subsection{결론}
본 과제의 목표는 Baseline 대비 도메인 일반화 성능을 향상시키면서도, 모델 구조나 계산 비용을 거의 늘리지 않는 방법을 찾는 것이었다. 실험 결과 FPJ-GC모듈이 이 요구를 가장 잘 충족했다. 따라서 FPJ-GC는 이론적 타당성과 일반화 성능을 모두 갖춘, 효과적인 도메인 일반화 솔루션으로 판단되어지며, 해당 모델을 우리들의 제안 모델로 선정하였다.

